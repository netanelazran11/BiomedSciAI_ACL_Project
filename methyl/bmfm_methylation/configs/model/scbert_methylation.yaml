# SCBert model configuration for methylation data
# Uses Flash Attention for memory efficiency with long sequences

_target_: bmfm_targets.config.SCBertConfig
_partial_: true

# Architecture
num_hidden_layers: 6
num_attention_heads: 8
hidden_size: 512
intermediate_size: 2048
hidden_act: "gelu"

# Regularization
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
classifier_dropout: 0.1

# Other
initializer_range: 0.02
layer_norm_eps: 1.0e-12
pad_token_id: 0
use_cache: true
max_position_embeddings: 8002  # 8000 CpG + [CLS] + [SEP]

# Use Flash Attention for memory efficiency
attention: "torch"

# Label columns (null for pretraining, set during fine-tuning)
label_columns: null

# Checkpoint (set during training)
checkpoint: ${checkpoint_path}
